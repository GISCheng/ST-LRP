{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from modules.sequential import Sequential\n",
    "from modules.linear import Linear\n",
    "from modules.softmax import Softmax\n",
    "from modules.relu import Relu\n",
    "from modules.tanh import Tanh\n",
    "from modules.convolution import Convolution\n",
    "from modules.avgpool import AvgPool\n",
    "from modules.maxpool import MaxPool\n",
    "from modules.utils import Utils, Summaries, plot_relevances\n",
    "import modules.render as render\n",
    "import examples.input_data as input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pdb\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import rcParams\n",
    "import pickle\n",
    "import copy\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "flags = tf.flags\n",
    "logging = tf.logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Input data\n",
    "\n",
    "rcParams['figure.figsize'] = 8, 10\n",
    "#128\n",
    "F1=open(\"E:/holiday_2_train_2016\",'rb')\n",
    "#100\n",
    "F2=open(\"E:/holiday_2_test_2016\",'rb')\n",
    "#50\n",
    "F3=open(\"E:/holiday_2_test_2017\",'rb')\n",
    "train=pickle.load(F1)\n",
    "validation=pickle.load(F2)\n",
    "test=pickle.load(F3)\n",
    "F1.close()\n",
    "F2.close()\n",
    "F3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Training parameters\n",
    "\n",
    "flags.DEFINE_integer(\"max_steps\", 30001,'Number of steps to run trainer.')\n",
    "flags.DEFINE_integer(\"batch_size\", 5,'Number of steps to run trainer.')\n",
    "flags.DEFINE_integer(\"test_every\", 100,'Number of steps to run trainer.')\n",
    "flags.DEFINE_float(\"learning_rate\", 1e-5,'Initial learning rate')\n",
    "flags.DEFINE_float(\"dropout\", 1, 'Keep probability for training dropout.')\n",
    "flags.DEFINE_boolean(\"relevance\", True,'Compute relevances')\n",
    "flags.DEFINE_string(\"relevance_method\", 'alphabeta','relevance methods: simple/epsilon/ww/flat/alphabeta')\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Network structure\n",
    "\n",
    "def nn():\n",
    "    \n",
    "    return Sequential([Convolution(weights_init= tf.truncated_normal_initializer(stddev=0.1),keep_prob=FLAGS.dropout,output_depth=50,input_depth=48,batch_size=FLAGS.batch_size,kernel_size=5,input_dim=30, act ='relu', stride_size=1, pad='SAME'),\n",
    "                       MaxPool(pad='SAME'),\n",
    "                       #AvgPool(pad='SAME'),\n",
    "                       \n",
    "                       Convolution(weights_init= tf.truncated_normal_initializer(stddev=0.1),keep_prob=FLAGS.dropout,kernel_size=4,output_depth=20,stride_size=1,batch_norm = False,batch_norm_params = {'momentum':0.9, 'epsilon':1e-3, 'training':True ,'name':'bn'},act ='relu', pad='SAME'),\n",
    "                       MaxPool(pad='SAME'),\n",
    "                       #AvgPool(pad='SAME'),\n",
    "                       \n",
    "                       Convolution(weights_init= tf.truncated_normal_initializer(stddev=0.1),keep_prob=FLAGS.dropout,kernel_size=4,output_depth=20,stride_size=1,batch_norm = True,batch_norm_params = {'momentum':0.9, 'epsilon':1e-3, 'training':True ,'name':'bn'},act ='relu', pad='SAME'),\n",
    "                       MaxPool(pad='SAME'),\n",
    "                       #AvgPool(pad='SAME'),\n",
    "                       \n",
    "                       #Linear(weights_init= tf.truncated_normal_initializer(stddev=0.1),input_dim=1600,output_dim=100,batch_norm = False, batch_norm_params = {'momentum':0.9, 'epsilon':1e-3, 'training':True,'name':'bn'}),\n",
    "                       Linear(weights_init= tf.truncated_normal_initializer(stddev=0.1),input_dim=320,output_dim=100,batch_norm = False, batch_norm_params = {'momentum':0.9, 'epsilon':1e-3, 'training':True,'name':'bn'}),\n",
    "                       Linear(weights_init= tf.truncated_normal_initializer(stddev=0.1),input_dim=100,output_dim=2),\n",
    "                       #Convolution(kernel_size=1, output_depth=12,stride_size=1, pad='VALID'),\n",
    "                       #Softmax()\n",
    "    ])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Model training or using this model\n",
    "\n",
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth = True\n",
    "sess=tf.Session(config=config)\n",
    "\n",
    "# Input placeholders\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, [None, 30,30,48], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, 2], name='y-input')\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "with tf.variable_scope('model'):\n",
    "    net = nn()\n",
    "    #inp = tf.pad(tf.reshape(x, [FLAGS.batch_size,30,30,1]),[[0,0],[2,2],[2,2],[0,0]])\n",
    "    inp = tf.reshape(x, [FLAGS.batch_size,30,30,48])\n",
    "    op = net.forward(inp)\n",
    "    y = tf.squeeze(op)\n",
    "    trainer = net.fit(output=y,ground_truth=y_,loss='softmax_crossentropy',optimizer='adam', opt_params=[FLAGS.learning_rate])\n",
    "with tf.variable_scope('relevance'):\n",
    "    if FLAGS.relevance:\n",
    "        LRP = net.lrp(op, FLAGS.relevance_method, 1)\n",
    "\n",
    "        # LRP layerwise \n",
    "        relevance_layerwise = []\n",
    "        R = op\n",
    "        for layer in net.modules[::-1]:\n",
    "            R = net.lrp_layerwise(layer, R, FLAGS.relevance_method, 1)\n",
    "            relevance_layerwise.append(R)\n",
    "\n",
    "    else:\n",
    "        LRP=[]\n",
    "        relevance_layerwise = []\n",
    "            \n",
    "with tf.name_scope('accuracy'):\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)), tf.float32))\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# Merge all the summaries\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "validation_x=list(np.array(validation)[:,0])\n",
    "validation_y=list(np.array(validation)[:,1])\n",
    "validation_acc = []\n",
    "train_acc = []\n",
    "validation_loss=[]\n",
    "\n",
    "##For trained model\n",
    "saver=tf.train.Saver()\n",
    "saver.restore(sess,'E:/holiday_2_200.cpk')\n",
    "\n",
    "##For model training\n",
    "\n",
    "#saver=tf.train.Saver(max_to_keep=1)\n",
    "# start=time.clock()\n",
    "# for i in range(FLAGS.max_steps):\n",
    "#     sample=random.sample(train,FLAGS.batch_size)\n",
    "#     batch_x=list(np.array(sample)[:,0])\n",
    "#     batch_y=list(np.array(sample)[:,1])\n",
    "        \n",
    "#     train_inp = {x:batch_x, y_: batch_y, keep_prob: FLAGS.dropout}\n",
    "#     summary, _ , train_accuracy, relevance_train,op, rel_layer,train_loss= sess.run([merged, trainer.train,accuracy, LRP,y, relevance_layerwise,trainer.cost], feed_dict=train_inp)\n",
    "#     if i % FLAGS.test_every==0:\n",
    "#         accuracys=[]\n",
    "#         for j in range(int(len(validation)/FLAGS.batch_size)):\n",
    "#             #sample_validation=random.sample(validation,FLAGS.batch_size)\n",
    "#             sample_validation=validation[j*FLAGS.batch_size:j*FLAGS.batch_size+FLAGS.batch_size]\n",
    "#             sv_x=list(np.array(sample_validation)[:,0])\n",
    "#             sv_y=list(np.array(sample_validation)[:,1])\n",
    "#             validation_inp = {x:sv_x, y_: sv_y, keep_prob: 1}\n",
    "#             #validation_inp = {x:validation_x, y_: validation_y, keep_prob: FLAGS.dropout}\n",
    "#             newaccuracy= sess.run([accuracy], feed_dict=validation_inp)\n",
    "#             accuracys.append(newaccuracy[0])\n",
    "#         validation_accuracy=sum(accuracys)/(len(validation)/FLAGS.batch_size)\n",
    "#         validation_acc.append(validation_accuracy)\n",
    "#         train_acc.append(train_accuracy)\n",
    "#         validation_loss.append(train_loss)\n",
    "#         print(str(i)+','+str(validation_accuracy)+','+str(train_accuracy)+','+str(train_loss))\n",
    "#         if(float(validation_accuracy)>0.95):\n",
    "#             break\n",
    "# endtime=time.clock()\n",
    "# print(endtime-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restore the trained network paremeters\n",
    "\n",
    "#saver.save(sess,'E:/holiday_2_200.cpk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Get the classification accuracy\n",
    "\n",
    "# accuracys=[]\n",
    "# for j in range(10):\n",
    "#     sample_test=test[j*5:j*5+5]\n",
    "#     st_x=list(np.array(sample_test)[:,0])\n",
    "#     st_y=list(np.array(sample_test)[:,1])\n",
    "#     test_inp = {x:st_x, y_: st_y, keep_prob: 1}\n",
    "#     newaccuracy= sess.run([accuracy], feed_dict=test_inp)\n",
    "#     print(newaccuracy[0])\n",
    "#     accuracys.append(newaccuracy[0])\n",
    "# test_accuracy=sum(accuracys)/10\n",
    "# print(str(test_accuracy))\n",
    "\n",
    "accuracys=[]\n",
    "for j in range(20):\n",
    "    sample_validation=validation[j*5:j*5+5]\n",
    "    sv_x=list(np.array(sample_validation)[:,0])\n",
    "    sv_y=list(np.array(sample_validation)[:,1])\n",
    "    validation_inp = {x:sv_x, y_: sv_y, keep_prob: 1}\n",
    "    newaccuracy= sess.run([accuracy], feed_dict=validation_inp)\n",
    "    print(newaccuracy[0])\n",
    "    accuracys.append(newaccuracy[0])\n",
    "validation_accuracy=sum(accuracys)/20\n",
    "print(str(validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Pick out the input with right classificated label\n",
    "\n",
    "F=open(\"E:/holiday_2_total_2016\",'rb')\n",
    "total2016=pickle.load(F)\n",
    "F.close()\n",
    "accuracys=[]\n",
    "rightrecord=[]\n",
    "for j in range(363):\n",
    "    sample_test=[]\n",
    "    for k in range(5):\n",
    "        sample_test.append(copy.deepcopy(total2016[j]))\n",
    "    st_x=list(np.array(sample_test)[:,0])\n",
    "    st_y=list(np.array(sample_test)[:,1])\n",
    "    test_inp = {x:st_x, y_: st_y, keep_prob: 1}\n",
    "    newaccuracy= sess.run([accuracy], feed_dict=test_inp)\n",
    "    if(int(newaccuracy[0])==1):\n",
    "        rightrecord.append(copy.deepcopy(total2016[j]))\n",
    "    accuracys.append(newaccuracy[0])\n",
    "test_accuracy=sum(accuracys)/363\n",
    "print(str(test_accuracy))\n",
    "W = open(\"E:/holiday_2_right_2016\", 'wb')\n",
    "pickle.dump(rightrecord,W,True)\n",
    "W.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Divide the input data by the label\n",
    "\n",
    "F=open(\"E:/holiday_2_right_2016\",'rb')\n",
    "totaldata=pickle.load(F)\n",
    "F.close()\n",
    "im_group1=[]\n",
    "im_group2=[]\n",
    "for i in range(len(totaldata)):\n",
    "    if(totaldata[i][1][0]==1):\n",
    "        im_group1.append(copy.deepcopy(totaldata[i][0]))\n",
    "    else:\n",
    "        im_group2.append(copy.deepcopy(totaldata[i][0]))\n",
    "print(len(im_group1))\n",
    "print(len(im_group2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calculate and restore the mean volume (Taxi origin points) of two categories (Weekday and Weekend/Holiday)\n",
    "\n",
    "mean1=np.mean(np.array(im_group1),axis=0)\n",
    "mean2=np.mean(np.array(im_group2),axis=0)\n",
    "M1 = open(\"E:/mean1\", 'wb')\n",
    "M2 = open(\"E:/mean2\", 'wb')\n",
    "pickle.dump(mean1,M1,True)\n",
    "pickle.dump(mean2,M2,True)\n",
    "M1.close()\n",
    "M2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calculate the heatmap (importance distribution) of two categories (Weekday and Weekend/Holiday)\n",
    "\n",
    "heat_group1=[]\n",
    "heat_group2=[]\n",
    "for i in range(len(totaldata)):\n",
    "    sample_test=[]\n",
    "    for n in range(5):\n",
    "        sample_test.append(copy.deepcopy(totaldata[i]))\n",
    "    ht_x=list(np.array(sample_test)[:,0])\n",
    "    ht_y=list(np.array(sample_test)[:,1])\n",
    "    test_inp = {x:ht_x, y_: ht_y, keep_prob: 1}\n",
    "    heatmap= sess.run([LRP], feed_dict=test_inp)\n",
    "    if(totaldata[i][1][0]==1):\n",
    "        heat_group1.append(copy.deepcopy(heatmap[0][0]))\n",
    "    else:\n",
    "        heat_group2.append(copy.deepcopy(heatmap[0][0]))\n",
    "print(len(heat_group1))\n",
    "print(len(heat_group2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Restore the heatmap (importance distribution) of two categories (Weekday and Weekend/Holiday)\n",
    "\n",
    "heat_mean1=np.mean(np.array(heat_group1),axis=0)\n",
    "heat_mean2=np.mean(np.array(heat_group2),axis=0)\n",
    "H1 = open(\"E:/heat_mean1\", 'wb')\n",
    "H2 = open(\"E:/heat_mean2\", 'wb')\n",
    "pickle.dump(heat_mean1,H1,True)\n",
    "pickle.dump(heat_mean2,H2,True)\n",
    "H1.close()\n",
    "H2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
