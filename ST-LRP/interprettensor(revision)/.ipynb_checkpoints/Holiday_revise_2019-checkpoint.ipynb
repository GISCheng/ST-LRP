{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from modules.sequential import Sequential\n",
    "from modules.linear import Linear\n",
    "from modules.softmax import Softmax\n",
    "from modules.relu import Relu\n",
    "from modules.tanh import Tanh\n",
    "from modules.convolution import Convolution\n",
    "from modules.avgpool import AvgPool\n",
    "from modules.maxpool import MaxPool\n",
    "from modules.utils import Utils, Summaries, plot_relevances\n",
    "import modules.render as render\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pdb\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import rcParams\n",
    "import pickle\n",
    "import copy\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "flags = tf.flags\n",
    "logging = tf.logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Input data\n",
    "\n",
    "rcParams['figure.figsize'] = 8, 10\n",
    "#128\n",
    "F1=open(\"E:/revisiondata/holiday_2_train_2016\",'rb')\n",
    "#100\n",
    "F2=open(\"E:/revisiondata/holiday_2_test_2016\",'rb')\n",
    "#50\n",
    "F3=open(\"E:/revisiondata/holiday_2_test_2017\",'rb')\n",
    "train=pickle.load(F1)\n",
    "validation=pickle.load(F2)\n",
    "test=pickle.load(F3)\n",
    "F1.close()\n",
    "F2.close()\n",
    "F3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Training parameters\n",
    "\n",
    "flags.DEFINE_integer(\"max_steps\", 30001,'Number of steps to run trainer.')\n",
    "flags.DEFINE_integer(\"batch_size\", 5,'Number of steps to run trainer.')\n",
    "flags.DEFINE_integer(\"test_every\", 100,'Number of steps to run trainer.')\n",
    "flags.DEFINE_float(\"learning_rate\", 1e-5,'Initial learning rate')\n",
    "flags.DEFINE_float(\"dropout\", 1, 'Keep probability for training dropout.')\n",
    "flags.DEFINE_boolean(\"relevance\", True,'Compute relevances')\n",
    "flags.DEFINE_string(\"relevance_method\", 'alphabeta','relevance methods: simple/epsilon/ww/flat/alphabeta')\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Network structure\n",
    "\n",
    "def nn():\n",
    "    \n",
    "    return Sequential([Convolution(weights_init= tf.truncated_normal_initializer(stddev=0.1),keep_prob=FLAGS.dropout,output_depth=50,input_depth=48,batch_size=FLAGS.batch_size,kernel_size=5,input_dim=30, act ='relu', stride_size=1, pad='SAME'),\n",
    "                       MaxPool(pad='SAME'),\n",
    "                       #AvgPool(pad='SAME'),\n",
    "                       \n",
    "                       Convolution(weights_init= tf.truncated_normal_initializer(stddev=0.1),keep_prob=FLAGS.dropout,kernel_size=4,output_depth=20,stride_size=1,batch_norm = False,batch_norm_params = {'momentum':0.9, 'epsilon':1e-3, 'training':True ,'name':'bn'},act ='relu', pad='SAME'),\n",
    "                       MaxPool(pad='SAME'),\n",
    "                       #AvgPool(pad='SAME'),\n",
    "                       \n",
    "                       Convolution(weights_init= tf.truncated_normal_initializer(stddev=0.1),keep_prob=FLAGS.dropout,kernel_size=4,output_depth=20,stride_size=1,batch_norm = True,batch_norm_params = {'momentum':0.9, 'epsilon':1e-3, 'training':True ,'name':'bn'},act ='relu', pad='SAME'),\n",
    "                       MaxPool(pad='SAME'),\n",
    "                       #AvgPool(pad='SAME'),\n",
    "                       \n",
    "                       #Linear(weights_init= tf.truncated_normal_initializer(stddev=0.1),input_dim=1600,output_dim=100,batch_norm = False, batch_norm_params = {'momentum':0.9, 'epsilon':1e-3, 'training':True,'name':'bn'}),\n",
    "                       Linear(weights_init= tf.truncated_normal_initializer(stddev=0.1),input_dim=320,output_dim=100,batch_norm = False, batch_norm_params = {'momentum':0.9, 'epsilon':1e-3, 'training':True,'name':'bn'}),\n",
    "                       Linear(weights_init= tf.truncated_normal_initializer(stddev=0.1),input_dim=100,output_dim=2),\n",
    "                       #Convolution(kernel_size=1, output_depth=12,stride_size=1, pad='VALID'),\n",
    "                       #Softmax()\n",
    "    ])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Pass ... \n",
      "------------------------------------------------- \n",
      "conv2d_1:: [5, 30, 30, 48]\n",
      "maxpool_2:: [5, 30, 30, 50]\n",
      "conv2d_3:: [5, 15, 15, 50]\n",
      "maxpool_4:: [5, 15, 15, 20]\n",
      "conv2d_5:: [5, 8, 8, 20]\n",
      "maxpool_6:: [5, 8, 8, 20]\n",
      "linear_7:: [5, 4, 4, 20]\n",
      "linear_8:: [5, 100]\n",
      "\n",
      "------------------------------------------------- \n",
      "Computing LRP ... \n",
      "------------------------------------------------- \n",
      "linear_8:: [5, 100]\n",
      "linear_7:: [5, 320]\n",
      "maxpool_6:: [5, 8, 8, 20]\n",
      "conv2d_5:: [5, 8, 8, 20]\n",
      "maxpool_4:: [5, 15, 15, 20]\n",
      "conv2d_3:: [5, 15, 15, 50]\n",
      "maxpool_2:: [5, 30, 30, 50]\n",
      "conv2d_1:: [5, 30, 30, 48]\n",
      "\n",
      "------------------------------------------------- \n",
      "linear_8:: [5, 100]\n",
      "linear_7:: [5, 320]\n",
      "maxpool_6:: [5, 8, 8, 20]\n",
      "conv2d_5:: [5, 8, 8, 20]\n",
      "maxpool_4:: [5, 15, 15, 20]\n",
      "conv2d_3:: [5, 15, 15, 50]\n",
      "maxpool_2:: [5, 30, 30, 50]\n",
      "conv2d_1:: [5, 30, 30, 48]\n",
      "INFO:tensorflow:Restoring parameters from E:/revisiondata/train/holiday_2_200.cpk\n"
     ]
    }
   ],
   "source": [
    "##Model training or using this model\n",
    "\n",
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth = True\n",
    "sess=tf.Session(config=config)\n",
    "\n",
    "# Input placeholders\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, [None, 30,30,48], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, 2], name='y-input')\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    #R = tf.placeholder(tf.float32, [None, 2])\n",
    "    \n",
    "with tf.variable_scope('model'):\n",
    "    net = nn()\n",
    "    #inp = tf.pad(tf.reshape(x, [FLAGS.batch_size,30,30,1]),[[0,0],[2,2],[2,2],[0,0]])\n",
    "    inp = tf.reshape(x, [FLAGS.batch_size,30,30,48])\n",
    "    op = net.forward(inp)\n",
    "    y = tf.squeeze(op)\n",
    "    trainer = net.fit(output=y,ground_truth=y_,loss='softmax_crossentropy',optimizer='adam', opt_params=[FLAGS.learning_rate])\n",
    "with tf.variable_scope('relevance'):\n",
    "    if FLAGS.relevance:\n",
    "        LRP = net.lrp(op, FLAGS.relevance_method, 1)\n",
    "\n",
    "        # LRP layerwise \n",
    "        relevance_layerwise = []\n",
    "        R = op\n",
    "        for layer in net.modules[::-1]:\n",
    "            R = net.lrp_layerwise(layer, R, FLAGS.relevance_method, 1)\n",
    "            relevance_layerwise.append(R)\n",
    "\n",
    "    else:\n",
    "        LRP=[]\n",
    "        relevance_layerwise = []\n",
    "            \n",
    "with tf.name_scope('accuracy'):\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)), tf.float32))\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# Merge all the summaries\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "validation_x=list(np.array(validation)[:,0])\n",
    "validation_y=list(np.array(validation)[:,1])\n",
    "validation_acc = []\n",
    "train_acc = []\n",
    "validation_loss=[]\n",
    "\n",
    "##For trained model\n",
    "saver=tf.train.Saver()\n",
    "#pretrained,30000\n",
    "saver.restore(sess,'E:/revisiondata/train/holiday_2_200.cpk')\n",
    "#saver.restore(sess,'E:/revisiondata/train/holiday_new_30000.cpk')\n",
    "\n",
    "#For model training\n",
    "\n",
    "# saver=tf.train.Saver(max_to_keep=1)\n",
    "# start=time.clock()\n",
    "# for i in range(FLAGS.max_steps):\n",
    "#     sample=random.sample(list(train),FLAGS.batch_size)\n",
    "#     batch_x=list(np.array(sample)[:,0])\n",
    "#     batch_y=list(np.array(sample)[:,1])\n",
    "        \n",
    "#     train_inp = {x:batch_x, y_: batch_y, keep_prob: FLAGS.dropout}\n",
    "#     summary, _ , train_accuracy, relevance_train,op, rel_layer,train_loss= sess.run([merged, trainer.train,accuracy, LRP,y, relevance_layerwise,trainer.cost], feed_dict=train_inp)\n",
    "#     if i % FLAGS.test_every==0:\n",
    "#         accuracys=[]\n",
    "#         for j in range(int(len(validation)/FLAGS.batch_size)):\n",
    "#             #sample_validation=random.sample(validation,FLAGS.batch_size)\n",
    "#             sample_validation=validation[j*FLAGS.batch_size:j*FLAGS.batch_size+FLAGS.batch_size]\n",
    "#             sv_x=list(np.array(sample_validation)[:,0])\n",
    "#             sv_y=list(np.array(sample_validation)[:,1])\n",
    "#             validation_inp = {x:sv_x, y_: sv_y, keep_prob: 1}\n",
    "#             #validation_inp = {x:validation_x, y_: validation_y, keep_prob: FLAGS.dropout}\n",
    "#             newaccuracy= sess.run([accuracy], feed_dict=validation_inp)\n",
    "#             accuracys.append(newaccuracy[0])\n",
    "#         validation_accuracy=sum(accuracys)/(len(validation)/FLAGS.batch_size)\n",
    "#         validation_acc.append(validation_accuracy)\n",
    "#         train_acc.append(train_accuracy)\n",
    "#         validation_loss.append(train_loss)\n",
    "#         print(str(i)+','+str(validation_accuracy)+','+str(train_accuracy)+','+str(train_loss))\n",
    "#         if(float(validation_accuracy)>0.95):\n",
    "#             break\n",
    "# endtime=time.clock()\n",
    "# print(endtime-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restore the trained network paremeters\n",
    "\n",
    "#saver.save(sess,'E:/revisiondata/train/holiday_new_30000.cpk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Get the classification accuracy\n",
    "\n",
    "# accuracys=[]\n",
    "# for j in range(10):\n",
    "#     sample_test=test[j*5:j*5+5]\n",
    "#     st_x=list(np.array(sample_test)[:,0])\n",
    "#     st_y=list(np.array(sample_test)[:,1])\n",
    "#     test_inp = {x:st_x, y_: st_y, keep_prob: 1}\n",
    "#     newaccuracy= sess.run([accuracy], feed_dict=test_inp)\n",
    "#     print(newaccuracy[0])\n",
    "#     accuracys.append(newaccuracy[0])\n",
    "# test_accuracy=sum(accuracys)/10\n",
    "# print(str(test_accuracy))\n",
    "\n",
    "accuracys=[]\n",
    "for j in range(20):\n",
    "    sample_validation=validation[j*5:j*5+5]\n",
    "    sv_x=list(np.array(sample_validation)[:,0])\n",
    "    sv_y=list(np.array(sample_validation)[:,1])\n",
    "    validation_inp = {x:sv_x, y_: sv_y, keep_prob: 1}\n",
    "    newaccuracy= sess.run([accuracy], feed_dict=validation_inp)\n",
    "    print(newaccuracy[0])\n",
    "    accuracys.append(newaccuracy[0])\n",
    "validation_accuracy=sum(accuracys)/20\n",
    "print(str(validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##importance distributions for two categories of data\n",
    "F=open(\"E:/revisiondata/holiday_2_total_2016\",'rb')\n",
    "total2016=pickle.load(F)\n",
    "F.close()\n",
    "heatmaps1=[]\n",
    "heatmaps2=[]\n",
    "for j in range(len(total2016)):\n",
    "    sample_test=[]\n",
    "    for k in range(FLAGS.batch_size):\n",
    "        sample_test.append(copy.deepcopy(total2016[j]))\n",
    "    st_x=list(np.array(sample_test)[:,0])\n",
    "    st_y=list(np.array(sample_test)[:,1])\n",
    "    test_inp = {x:st_x, y_: st_y, keep_prob: 1}\n",
    "    newaccuracy= sess.run([accuracy], feed_dict=test_inp)\n",
    "    if(int(newaccuracy[0])==1):\n",
    "        heat_inp={x:st_x, y_: st_y, keep_prob: 1,op: st_y}\n",
    "        tmpheatmap,tmpy=sess.run([LRP,y], feed_dict=heat_inp)\n",
    "        resultheatmap=copy.deepcopy(tmpheatmap[0])\n",
    "        resulty=copy.deepcopy(tmpy[0])\n",
    "        if(resulty[0]>=resulty[1]):\n",
    "            heatmaps1.append(resultheatmap)\n",
    "        else:\n",
    "            heatmaps2.append(resultheatmap)\n",
    "\n",
    "W = open(\"E:/revisiondata/heatmaps1\", 'wb')\n",
    "pickle.dump(heatmaps1,W,True)\n",
    "W.close()\n",
    "W = open(\"E:/revisiondata/heatmaps2\", 'wb')\n",
    "pickle.dump(heatmaps2,W,True)\n",
    "W.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##importance distributions for two categories of data (only samples)\n",
    "F=open(\"E:/revisiondata/holiday_2_total_2016\",'rb')\n",
    "total2016=pickle.load(F)\n",
    "F.close()\n",
    "heatmaps1=[]\n",
    "heatmaps2=[]\n",
    "#samples' ID with high values in both output elements\n",
    "judgeID=[36,37,44,78,122,156,279,313,348]\n",
    "for i in judgeID:\n",
    "    sample_test=[]\n",
    "    for k in range(FLAGS.batch_size):\n",
    "        sample_test.append(copy.deepcopy(total2016[i]))\n",
    "    st_x=list(np.array(sample_test)[:,0])\n",
    "    st_y=list(np.array(sample_test)[:,1])\n",
    "    label2=[[0,1],[0,1],[0,1],[0,1],[0,1]]\n",
    "    label1=[[1,0],[1,0],[1,0],[1,0],[1,0]]\n",
    "    test_inp_1 = {x:st_x, y_: st_y, keep_prob: 1,op: label1}\n",
    "    test_inp_2 = {x:st_x, y_: st_y, keep_prob: 1,op: label2}\n",
    "    heatmap1= sess.run([LRP], feed_dict=test_inp_1)\n",
    "    heatmap2= sess.run([LRP], feed_dict=test_inp_2)\n",
    "    heatmaps1.append(copy.deepcopy(heatmap1[0][0]))\n",
    "    heatmaps2.append(copy.deepcopy(heatmap2[0][0]))\n",
    "\n",
    "W = open(\"E:/revisiondata/sample_heatmaps1\", 'wb')\n",
    "pickle.dump(heatmaps1,W,True)\n",
    "W.close()\n",
    "W = open(\"E:/revisiondata/sample_heatmaps2\", 'wb')\n",
    "pickle.dump(heatmaps2,W,True)\n",
    "W.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Pick out the input with right classificated label\n",
    "\n",
    "F=open(\"E:/holiday_2_total_2016\",'rb')\n",
    "total2016=pickle.load(F)\n",
    "F.close()\n",
    "accuracys=[]\n",
    "rightrecord=[]\n",
    "for j in range(363):\n",
    "    sample_test=[]\n",
    "    for k in range(FLAGS.batch_size):\n",
    "        sample_test.append(copy.deepcopy(total2016[j]))\n",
    "    st_x=list(np.array(sample_test)[:,0])\n",
    "    st_y=list(np.array(sample_test)[:,1])\n",
    "    test_inp = {x:st_x, y_: st_y, keep_prob: 1}\n",
    "    newaccuracy= sess.run([accuracy], feed_dict=test_inp)\n",
    "    if(int(newaccuracy[0])==1):\n",
    "        rightrecord.append(copy.deepcopy(total2016[j]))\n",
    "    accuracys.append(newaccuracy[0])\n",
    "test_accuracy=sum(accuracys)/363\n",
    "print(str(test_accuracy))\n",
    "W = open(\"E:/holiday_2_right_2016\", 'wb')\n",
    "pickle.dump(rightrecord,W,True)\n",
    "W.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Divide the input data by the label\n",
    "\n",
    "F=open(\"E:/holiday_2_right_2016\",'rb')\n",
    "totaldata=pickle.load(F)\n",
    "F.close()\n",
    "im_group1=[]\n",
    "im_group2=[]\n",
    "for i in range(len(totaldata)):\n",
    "    if(totaldata[i][1][0]==1):\n",
    "        im_group1.append(copy.deepcopy(totaldata[i][0]))\n",
    "    else:\n",
    "        im_group2.append(copy.deepcopy(totaldata[i][0]))\n",
    "print(len(im_group1))\n",
    "print(len(im_group2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calculate and restore the mean volume (Taxi origin points) of two categories (Weekday and Weekend/Holiday)\n",
    "\n",
    "mean1=np.mean(np.array(im_group1),axis=0)\n",
    "mean2=np.mean(np.array(im_group2),axis=0)\n",
    "M1 = open(\"E:/mean1\", 'wb')\n",
    "M2 = open(\"E:/mean2\", 'wb')\n",
    "pickle.dump(mean1,M1,True)\n",
    "pickle.dump(mean2,M2,True)\n",
    "M1.close()\n",
    "M2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Calculate the heatmap (importance distribution) of two categories (Weekday and Weekend/Holiday)\n",
    "\n",
    "heat_group1=[]\n",
    "heat_group2=[]\n",
    "for i in range(len(totaldata)):\n",
    "    sample_test=[]\n",
    "    for n in range(5):\n",
    "        sample_test.append(copy.deepcopy(totaldata[i]))\n",
    "    ht_x=list(np.array(sample_test)[:,0])\n",
    "    ht_y=list(np.array(sample_test)[:,1])\n",
    "    test_inp = {x:ht_x, y_: ht_y, keep_prob: 1}\n",
    "    heatmap= sess.run([LRP], feed_dict=test_inp)\n",
    "    if(totaldata[i][1][0]==1):\n",
    "        heat_group1.append(copy.deepcopy(heatmap[0][0]))\n",
    "    else:\n",
    "        heat_group2.append(copy.deepcopy(heatmap[0][0]))\n",
    "print(len(heat_group1))\n",
    "print(len(heat_group2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Restore the heatmap (importance distribution) of two categories (Weekday and Weekend/Holiday)\n",
    "\n",
    "heat_mean1=np.mean(np.array(heat_group1),axis=0)\n",
    "heat_mean2=np.mean(np.array(heat_group2),axis=0)\n",
    "H1 = open(\"E:/heat_mean1\", 'wb')\n",
    "H2 = open(\"E:/heat_mean2\", 'wb')\n",
    "pickle.dump(heat_mean1,H1,True)\n",
    "pickle.dump(heat_mean2,H2,True)\n",
    "H1.close()\n",
    "H2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
