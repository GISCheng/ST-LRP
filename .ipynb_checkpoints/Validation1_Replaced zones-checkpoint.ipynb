{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from modules.sequential import Sequential\n",
    "from modules.linear import Linear\n",
    "from modules.softmax import Softmax\n",
    "from modules.relu import Relu\n",
    "from modules.tanh import Tanh\n",
    "from modules.convolution import Convolution\n",
    "from modules.avgpool import AvgPool\n",
    "from modules.maxpool import MaxPool\n",
    "from modules.utils import Utils, Summaries, plot_relevances\n",
    "import modules.render as render\n",
    "import examples.input_data as input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pdb\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import rcParams\n",
    "import pickle\n",
    "import copy\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "flags = tf.flags\n",
    "logging = tf.logging\n",
    "\n",
    "rcParams['figure.figsize'] = 8, 10\n",
    "#128\n",
    "F1=open(\"E:/holiday_2_train_2016\",'rb')\n",
    "#100\n",
    "F2=open(\"E:/holiday_2_test_2016\",'rb')\n",
    "#50\n",
    "F3=open(\"E:/holiday_2_test_2017\",'rb')\n",
    "train=pickle.load(F1)\n",
    "validation=pickle.load(F2)\n",
    "test=pickle.load(F3)\n",
    "F1.close()\n",
    "F2.close()\n",
    "F3.close()\n",
    "\n",
    "flags.DEFINE_integer(\"max_steps\", 30001,'Number of steps to run trainer.')\n",
    "flags.DEFINE_integer(\"batch_size\", 5,'Number of steps to run trainer.')\n",
    "flags.DEFINE_integer(\"test_every\", 100,'Number of steps to run trainer.')\n",
    "flags.DEFINE_float(\"learning_rate\", 1e-5,'Initial learning rate')\n",
    "flags.DEFINE_float(\"dropout\", 1, 'Keep probability for training dropout.')\n",
    "flags.DEFINE_boolean(\"relevance\", True,'Compute relevances')\n",
    "flags.DEFINE_string(\"relevance_method\", 'alphabeta','relevance methods: simple/epsilon/ww/flat/alphabeta')\n",
    "\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "def nn():\n",
    "    \n",
    "    return Sequential([Convolution(weights_init= tf.truncated_normal_initializer(stddev=0.1),keep_prob=FLAGS.dropout,output_depth=50,input_depth=48,batch_size=FLAGS.batch_size,kernel_size=5,input_dim=30, act ='relu', stride_size=1, pad='SAME'),\n",
    "                       MaxPool(pad='SAME'),\n",
    "                       #AvgPool(pad='SAME'),\n",
    "                       \n",
    "                       Convolution(weights_init= tf.truncated_normal_initializer(stddev=0.1),keep_prob=FLAGS.dropout,kernel_size=4,output_depth=20,stride_size=1,batch_norm = False,batch_norm_params = {'momentum':0.9, 'epsilon':1e-3, 'training':True ,'name':'bn'},act ='relu', pad='SAME'),\n",
    "                       MaxPool(pad='SAME'),\n",
    "                       #AvgPool(pad='SAME'),\n",
    "                       \n",
    "                       Convolution(weights_init= tf.truncated_normal_initializer(stddev=0.1),keep_prob=FLAGS.dropout,kernel_size=4,output_depth=20,stride_size=1,batch_norm = True,batch_norm_params = {'momentum':0.9, 'epsilon':1e-3, 'training':True ,'name':'bn'},act ='relu', pad='SAME'),\n",
    "                       MaxPool(pad='SAME'),\n",
    "                       #AvgPool(pad='SAME'),\n",
    "                       \n",
    "                       Linear(weights_init= tf.truncated_normal_initializer(stddev=0.1),input_dim=320,output_dim=100,batch_norm = False, batch_norm_params = {'momentum':0.9, 'epsilon':1e-3, 'training':True,'name':'bn'}),\n",
    "                       \n",
    "                       Linear(weights_init= tf.truncated_normal_initializer(stddev=0.1),input_dim=100,output_dim=2, act ='relu'),\n",
    "                       #Convolution(kernel_size=1, output_depth=12,stride_size=1, pad='VALID'),\n",
    "                       #Softmax()\n",
    "    ])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Using this model\n",
    "\n",
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto(allow_soft_placement = True)\n",
    "config.gpu_options.allow_growth = True\n",
    "sess=tf.Session(config=config)\n",
    "\n",
    "# Input placeholders\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32, [None, 30,30,48], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, 2], name='y-input')\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "with tf.variable_scope('model'):\n",
    "    net = nn()\n",
    "    #inp = tf.pad(tf.reshape(x, [FLAGS.batch_size,30,30,1]),[[0,0],[2,2],[2,2],[0,0]])\n",
    "    inp = tf.reshape(x, [FLAGS.batch_size,30,30,48])\n",
    "    op = net.forward(inp)\n",
    "    y = tf.squeeze(op)\n",
    "    trainer = net.fit(output=y,ground_truth=y_,loss='softmax_crossentropy',optimizer='adam', opt_params=[FLAGS.learning_rate])\n",
    "with tf.variable_scope('relevance'):\n",
    "    if FLAGS.relevance:\n",
    "        LRP = net.lrp(op, FLAGS.relevance_method, 1)\n",
    "\n",
    "        # LRP layerwise \n",
    "        relevance_layerwise = []\n",
    "        R = op\n",
    "        for layer in net.modules[::-1]:\n",
    "            R = net.lrp_layerwise(layer, R, FLAGS.relevance_method, 1)\n",
    "            relevance_layerwise.append(R)\n",
    "\n",
    "    else:\n",
    "        LRP=[]\n",
    "        relevance_layerwise = []\n",
    "            \n",
    "with tf.name_scope('accuracy'):\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)), tf.float32))\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# Merge all the summaries\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "    \n",
    "validation_x=list(np.array(validation)[:,0])\n",
    "validation_y=list(np.array(validation)[:,1])\n",
    "validation_acc = []\n",
    "train_acc = []\n",
    "validation_loss=[]\n",
    "#saver=tf.train.Saver(max_to_keep=1)\n",
    "\n",
    "\n",
    "##For trained model\n",
    "saver=tf.train.Saver()\n",
    "saver.restore(sess,'E:/holiday_2_200.cpk')\n",
    "\n",
    "##For model training\n",
    "# start=time.clock()\n",
    "# for i in range(FLAGS.max_steps):\n",
    "#     sample=random.sample(train,FLAGS.batch_size)\n",
    "#     batch_x=list(np.array(sample)[:,0])\n",
    "#     batch_y=list(np.array(sample)[:,1])\n",
    "        \n",
    "#     train_inp = {x:batch_x, y_: batch_y, keep_prob: FLAGS.dropout}\n",
    "#     summary, _ , train_accuracy, relevance_train,op, rel_layer,train_loss= sess.run([merged, trainer.train,accuracy, LRP,y, relevance_layerwise,trainer.cost], feed_dict=train_inp)\n",
    "#     if i % FLAGS.test_every==0:\n",
    "#         accuracys=[]\n",
    "#         for j in range(int(len(validation)/FLAGS.batch_size)):\n",
    "#             #sample_validation=random.sample(validation,FLAGS.batch_size)\n",
    "#             sample_validation=validation[j*FLAGS.batch_size:j*FLAGS.batch_size+FLAGS.batch_size]\n",
    "#             sv_x=list(np.array(sample_validation)[:,0])\n",
    "#             sv_y=list(np.array(sample_validation)[:,1])\n",
    "#             validation_inp = {x:sv_x, y_: sv_y, keep_prob: 1}\n",
    "#             #validation_inp = {x:validation_x, y_: validation_y, keep_prob: FLAGS.dropout}\n",
    "#             newaccuracy= sess.run([accuracy], feed_dict=validation_inp)\n",
    "#             accuracys.append(newaccuracy[0])\n",
    "#         validation_accuracy=sum(accuracys)/(len(validation)/FLAGS.batch_size)\n",
    "#         validation_acc.append(validation_accuracy)\n",
    "#         train_acc.append(train_accuracy)\n",
    "#         validation_loss.append(train_loss)\n",
    "#         print(str(i)+','+str(validation_accuracy)+','+str(train_accuracy)+','+str(train_loss))\n",
    "#         if(float(validation_accuracy)>0.95):\n",
    "#             break\n",
    "# endtime=time.clock()\n",
    "# print(endtime-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##The assessed importance order of each spatial unit\n",
    "\n",
    "order=[\n",
    "    429,524,766,321,262,381,383,525,382,320,850,289,351,350,378,377,384,261,408,745,322,374,232,356,231,230,430,409,290,282,806,438,\n",
    "    790,174,441,459,319,523,744,288,411,373,368,776,432,431,260,113,372,343,323,380,442,258,325,555,197,173,371,775,465,807,667,556,\n",
    "    492,494,439,463,112,385,312,836,144,466,348,349,342,291,379,554,399,129,204,501,879,557,201,433,365,614,200,170,402,584,326,386,\n",
    "    254,500,159,553,428,517,529,347,338,471,357,559,101,531,142,199,434,583,229,198,143,367,440,216,224,318,366,522,188,403,175,460,\n",
    "    222,228,353,195,738,743,404,190,585,493,309,311,248,168,458,217,196,436,369,158,286,558,280,489,283,352,161,496,740,194,528,355,\n",
    "    108,820,234,798,128,77,162,256,413,462,414,443,317,706,341,252,306,774,805,586,622,504,472,223,316,495,741,138,220,172,284,314,\n",
    "    191,736,235,395,257,760,425,407,278,437,878,768,287,337,133,310,107,387,400,461,637,405,589,464,490,470,145,703,218,281,82,295,\n",
    "    410,671,139,227,354,865,804,468,415,250,412,100,259,221,527,336,445,535,324,427,715,899,444,164,455,502,526,294,114,165,473,130,\n",
    "    487,607,606,406,560,308,189,109,486,615,532,344,52,868,469,435,565,849,538,518,676,566,166,83,491,339,533,249,761,296,134,279,\n",
    "    416,617,582,335,305,315,424,111,644,396,638,579,388,389,401,131,266,247,837,398,370,548,219,686,894,255,167,313,160,536,78,137,\n",
    "    763,547,497,364,739,203,426,588,135,581,446,251,132,838,169,205,613,208,193,192,747,829,519,85,666,187,880,576,773,636,253,75,\n",
    "    163,397,516,503,587,787,505,748,521,293,233,515,99,731,346,580,746,498,882,530,546,835,655,297,791,687,215,537,711,392,620,643,\n",
    "    851,334,186,860,245,345,76,340,393,105,104,456,71,474,697,57,848,564,866,590,823,72,762,141,650,477,84,895,567,376,737,716,\n",
    "    881,103,307,269,616,563,277,817,171,226,717,98,375,102,641,358,106,157,275,733,304,292,457,417,394,69,276,74,710,562,97,608,\n",
    "    611,115,549,734,467,819,646,653,681,764,602,767,86,701,42,202,520,898,618,263,53,593,625,609,652,447,714,597,391,685,246,777,\n",
    "    803,719,56,821,645,572,771,545,867,897,669,735,239,422,359,854,853,488,797,507,539,81,87,809,642,552,551,592,619,712,578,859,\n",
    "    639,238,869,264,683,544,893,110,140,327,577,514,772,651,244,756,225,896,484,677,730,361,423,679,66,718,689,786,55,209,709,298,\n",
    "    623,840,212,27,640,670,44,824,499,534,362,67,883,41,73,303,770,688,136,596,328,759,285,626,864,54,890,612,808,792,668,482,\n",
    "    360,51,561,485,852,70,889,116,513,742,418,863,543,154,575,769,834,857,185,649,627,79,861,855,732,454,672,483,363,542,267,678,\n",
    "    624,799,22,870,648,333,704,673,206,707,727,633,660,419,178,237,476,68,684,39,845,621,243,302,876,875,599,265,647,12,390,789,\n",
    "    758,47,448,48,207,176,449,598,862,682,153,25,156,117,509,690,301,9,796,856,96,726,713,421,43,700,603,452,26,698,757,610,\n",
    "    5,788,779,568,674,839,273,605,778,822,841,183,858,18,14,45,696,299,794,236,6,708,833,811,211,574,179,659,594,591,630,35,\n",
    "    844,828,124,37,749,816,123,330,420,595,478,628,146,65,506,59,332,665,34,21,512,658,657,155,765,571,40,702,177,573,19,600,\n",
    "    151,729,20,604,884,886,90,654,300,268,830,152,801,24,570,49,846,63,270,17,213,7,479,699,720,481,569,818,58,50,793,802,\n",
    "    118,13,28,242,891,887,127,842,453,826,91,29,46,64,214,827,36,871,847,814,656,150,508,126,635,810,240,785,475,815,271,23,\n",
    "    274,550,728,885,11,241,750,629,751,62,147,680,755,843,705,210,825,877,892,88,694,675,329,632,888,8,184,180,795,874,800,120,\n",
    "    725,121,272,634,780,664,122,541,16,10,691,61,80,181,721,784,812,511,450,451,331,15,663,93,510,149,33,148,94,89,119,540,\n",
    "    92,125,695,781,601,831,724,60,832,480,95,182,754,813,723,753,722,783,782,693,692,38,631,30,872,873,662,4,661,1,2,32,\n",
    "    3,31,0,752\n",
    "]\n",
    "groupnum=[9,24,45,74,111,156,211,282,387]\n",
    "def getrc(n):\n",
    "    a=int(n/30)\n",
    "    b=n%30\n",
    "    return a,b\n",
    "\n",
    "F=open(\"E:/holiday_2_total_2016\",'rb')\n",
    "total2016=pickle.load(F)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##Validation #1, replaced the values of several zones and get the classification accuracy\n",
    "\n",
    "for v in range(len(groupnum)):\n",
    "    replacenum=groupnum[v]\n",
    "    totalaccuracy=[]\n",
    "    for repeat in range(50):\n",
    "        accuracys=[]\n",
    "        for j in range(len(total2016)):\n",
    "            sample_test=[]\n",
    "            tmpimage=copy.deepcopy(total2016[j])\n",
    "        \n",
    "            hourimg=np.array(tmpimage[0]).reshape(900,48)\n",
    "            hourmean=random.sample(list(hourimg),1)\n",
    "            #hourmean=np.mean(np.mean(np.array(tmpimage[0]),axis=0),axis=0)\n",
    "            \n",
    "            baseline=random.sample(order,replacenum)\n",
    "        \n",
    "            for t in range(replacenum):\n",
    "                rnum, cnum=getrc(order[t])\n",
    "                #rnum,cnum=getrc(baseline[t])\n",
    "                for d in range(48):\n",
    "                    #tmpimage[0][rnum][cnum][d]=0\n",
    "                    tmpimage[0][rnum][cnum][d]=hourmean[d] \n",
    "        \n",
    "            for k in range(5):\n",
    "                sample_test.append(tmpimage)\n",
    "            st_x=list(np.array(sample_test)[:,0])\n",
    "            st_y=list(np.array(sample_test)[:,1])\n",
    "            test_inp = {x:st_x, y_: st_y, keep_prob: 1}\n",
    "            newaccuracy= sess.run([accuracy], feed_dict=test_inp)\n",
    "            accuracys.append(newaccuracy[0])\n",
    "        test_accuracy=sum(accuracys)/len(total2016)\n",
    "        totalaccuracy.append(test_accuracy)\n",
    "    #print(totalaccuracy)\n",
    "    print(np.mean(totalaccuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
